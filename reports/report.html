<h1>Exam template for 02476 Machine Learning Operations</h1>
<p>This is the report template for the exam. Please only remove the text formatted as with three dashes in front and behind
like:</p>
<p>"  --- question 1 fill here ---"  </p>
<p>where you instead should add your answers. Any other changes may have unwanted consequences when your report is auto
generated in the end of the course. For questions where you are asked to include images, start by adding the image to
the <code>figures</code> subfolder (please only use <code>.png</code>, <code>.jpg</code> or <code>.jpeg</code>) and then add the following code in your answer:</p>
<!-- "  markdown
![my_image](figures/<image.<extension)
"   -->

<p>In addition to this markdown file, we also provide the <code>report.py</code> script that provides two utility functions:</p>
<p>Running:</p>
<p>"  bash
python report.py html
"  </p>
<p>will generate an <code>.html</code> page of your report. After deadline for answering this template, we will autoscrape
everything in this <code>reports</code> folder and then use this utility to generate an <code>.html</code> page that will be your serve
as your final handin.</p>
<p>Running</p>
<p>"  bash
python report.py check
"  </p>
<p>will check your answers in this template against the constrains listed for each question e.g. is your answer too
short, too long, have you included an image when asked to.</p>
<p>For both functions to work it is important that you do not rename anything. The script have two dependencies that can
be installed with <code>pip install click markdown</code>.</p>
<h2>Overall project checklist</h2>
<p>The checklist is <em>exhaustic</em> which means that it includes everything that you could possible do on the project in
relation the curricilum in this course. Therefore, we do not expect at all that you have checked of all boxes at the
end of the project.</p>
<h3>Week 1</h3>
<ul>
<li>[X] Create a git repository</li>
<li>[X] Make sure that all team members have write access to the github repository</li>
<li>[X] Create a dedicated environment for you project to keep track of your packages</li>
<li>[X] Create the initial file structure using cookiecutter</li>
<li>[X] Fill out the <code>make_dataset.py</code> file such that it downloads whatever data you need and</li>
<li>[X] Add a model file and a training script and get that running</li>
<li>[X] Remember to fill out the <code>requirements.txt</code> file with whatever dependencies that you are using</li>
<li>[X] Remember to comply with good coding practices (<code>pep8</code>) while doing the project</li>
<li>[X] Do a bit of code typing and remember to document essential parts of your code</li>
<li>[X] Setup version control for your data or part of your data</li>
<li>[X] Construct one or multiple docker files for your code</li>
<li>[X] Build the docker files locally and make sure they work as intended</li>
<li>[X] Write one or multiple configurations files for your experiments</li>
<li>[X] Used Hydra to load the configurations and manage your hyperparameters</li>
<li>[X] When you have something that works somewhat, remember at some point to to some profiling and see if
      you can optimize your code</li>
<li>[X] Use Weights &amp; Biases to log training progress and other important metrics/artifacts in your code. Additionally,
      consider running a hyperparameter optimization sweep.</li>
<li>[X] Use Pytorch-lightning (if applicable) to reduce the amount of boilerplate in your code</li>
</ul>
<h3>Week 2</h3>
<ul>
<li>[X] Write unit tests related to the data part of your code</li>
<li>[X] Write unit tests related to model construction and or model training</li>
<li>[X] Calculate the coverage.</li>
<li>[X] Get some continuous integration running on the github repository</li>
<li>[X] Create a data storage in GCP Bucket for you data and preferable link this with your data version control setup</li>
<li>[X] Create a trigger workflow for automatically building your docker images</li>
<li>[X] Get your model training in GCP using either the Engine or Vertex AI</li>
<li>[X] Create a FastAPI application that can do inference using your model</li>
<li>[ ] If applicable, consider deploying the model locally using torchserve</li>
<li>[X] Deploy your model in GCP using either Functions or Run as the backend</li>
</ul>
<h3>Week 3</h3>
<ul>
<li>[ ] Check how robust your model is towards data drifting</li>
<li>[ ] Setup monitoring for the system telemetry of your deployed model</li>
<li>[ ] Setup monitoring for the performance of your deployed model</li>
<li>[ ] If applicable, play around with distributed data loading</li>
<li>[ ] If applicable, play around with distributed model training</li>
<li>[ ] Play around with quantization, compilation and pruning for you trained models to increase inference speed</li>
</ul>
<h3>Additional</h3>
<ul>
<li>[X] Revisit your initial project description. Did the project turn out as you wanted?</li>
<li>[X] Make sure all group members have a understanding about all parts of the project</li>
<li>[X] Uploaded all your code to github</li>
</ul>
<h2>Group information</h2>
<h3>Question 1</h3>
<p><strong>Enter the group number you signed up on &lt;learn.inside.dtu.dk</strong></p>
<p>Answer:</p>
<p>MLOps 13</p>
<h3>Question 2</h3>
<p><strong>Enter the study number for each member in the group</strong></p>
<p>Answer:</p>
<p>s223596, s222720, s202075, s230883</p>
<h3>Question 3</h3>
<p><strong>What framework did you choose to work with and did it help you complete the project?</strong></p>
<p>Answer:</p>
<p>We used the third-party framework Pytorch, more specifically Pytorch-Lightning, in our project. We utilized the Torchvision pretrained VGG16 model, incorporating default weights derived from the ImageNet challenge, as a base for our model where the output layer of the classifier part was changed from 1000 to 8 which the number of our classes. For data augmentation we used transforms module from torchvision. From Pytorch-lightning we used their Trainer class to train, validate and test our model and integrated result logger. The Adam optimizer from PyTorch's optimizers, in conjunction with the Cross Entropy Loss from torch.nn, comprised our chosen optimization strategy. Dataset and DataLoader class from pytorch were used to create customized data loader.</p>
<h2>Coding environment</h2>
<p>In the following section we are interested in learning more about you local development environment.</p>
<h3>Question 4</h3>
<p><strong>Explain how you managed dependencies in your project? Explain the process a new team member would have to go</strong>
 <strong>through to get an exact copy of your environment.</strong></p>
<p>Answer:</p>
<p>We used a simple requirements.txt file stored in our root of the GitHub repository to manage our dependencies which was updated by team members. <br />
To get a complete copy of our development environment one would need get accounts at websites and access to the following teams 
Weights and Biases : dtu_mlops_group13<br />
Google Cloud : mlops-group13<br />
GitHub repository : https://github.com/magnussig/ml-ops  </p>
<p>Assuming that you already have <br />
python 3.10 installed and the command “python” in the   command line points to it. <br />
gnu make installed  </p>
<p>Get the github repository<br />
<code >
    <span style="background-color:#eee;">
    git clone https://github.com/magnussig/ml-ops.git  <br />
    </span>
</code></p>
<p>Create virtual development environment <br />
With the following commands   <br />
<code >
    <span style="background-color:#eee;">
        cd ml-ops    <br />
        python -m venv venv
    </span>
</code></p>
<p>Activate virtual environment in Windows  <br />
<code><br />
    <span style="background-color:#eee;size:15">
        venv\Scripts\activate<br />
    </span><br />
</code ></p>
<p>For Linux/Unix <br />
<code >
    <span style="background-color:#eee;">
        source venv/bin/activate    <br />
    </span><br />
<code></p>
<p>Install packages<br />
<code >
    <span style="background-color:#eee;">
    make requirements   <br />
    </span>
</code></p>
<p>NOTE: This uses the CPU version of pytorch. See the following for the GPU version : https://pytorch.org/get-started/locally/  </p>
<p>To get raw data and put in correct format<br />
<code >
    <span style="background-color:#eee;">
    make data<br />
    </span>
</code></p>
<h3>Question 5</h3>
<p><strong>We expect that you initialized your project using the cookiecutter template. Explain the overall structure of your</strong>
 <strong>code. Did you fill out every folder or only a subset?</strong></p>
<p>Answer:</p>
<p>This is our final structure  </p>
<!-- ![Folder structure](figures/folder_structure.png) -->
<p><img src="figures/folder_structure.png" alt="drawing" width="300"/></p>
<p>Starting from the top. </p>
<p>data:<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contains two folders, raw which contains all the original images and folders and processed<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; which has the processed
    Images</p>
<p>docs: <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contains information and sources for to create the documentation</p>
<p>models: <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Local storage for a model who performed the best in the training run</p>
<p>Reports: <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contains information and scripts who create the report from file</p>
<p>Src:<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Main code folder.<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; conf folder contains the hydra configurations<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data contains script that processes the data from raw to processed<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Models folder contains the model architectures<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Api.py contains code the FastAPI code<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data_loader.py is the code that creates the data loaders <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Train_model.py is the code containing the training, validation and testing of the model<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Upload_file.py uploads the model from models folder to cloud.</p>
<p>Tests:<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Contains all our tests which are divided into two. One for the data and data loaders, <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and the other for the models and training. </p>
<h3>Question 6</h3>
<p><strong>Did you implement any rules for code quality and format? Additionally, explain with your own words why these</strong>
 <strong>concepts matters in larger projects.</strong></p>
<p>Answer:
We have a GitHub workflow that runs Ruff on every push we make, if the job fails we see it there and can fix it. We also contemplated implementing a pre-commit hook but didn’t have time to add it to the project.<br />
Also mypy is used for type checking, and similar to ruff, its success or failure will be shown along with a sufficient message to find the error (if exists) and fix it.<br />
The latest commit on GitHub passes both checks.</p>
<h2>Version control</h2>
<p>In the following section we are interested in how version control was used in your project during development to corporate and increase the quality of your code.</p>
<h3>Question 7</h3>
<p><strong>How many tests did you implement and what are they testing in your code?</strong></p>
<p>Answer:
We implemented 14 total tests, for data_loading, our custom model, a pretrained model and the data itself. The tests for the data_loader and the data are meant to be run by the developer and the rest are run in a GitHub workflow.<br />
The data tests are primarily for the developer to check if their data is in the correct folder structure, all the raw images were processed correctly and the data loaders are working as intended.<br />
The other tests are to check if the models is working and logging as intended. </p>
<h3>Question 8</h3>
<p><strong>What is the total code coverage (in percentage) of your code? If you code had an code coverage of 100% (or close</strong>
 <strong>to), would you still trust it to be error free? Explain you reasoning.</strong></p>
<p>Answer:<br />
 The total coverage is around 77%, which relatively is a good coverage, but even if it was 100%, it doesn’t guarantee a bug-free application. The reason is that it only says how many lines of code are getting tested, but doesn’t check if all different scenarios are being tested or not. An example is a case in which a function works buggy on a special input, but that input is never present in test condition. In this case the coverage is 100%, but the code is buggy. Or a more related example is that we check if our training function works or not, but because of lack of time we didn’t check if the model is actually improving in the course of training or not.</p>
<h3>Question 9</h3>
<p><strong>Did you workflow include using branches and pull requests? If yes, explain how. If not, explain how branches and</strong>
 <strong>pull request can help improve version control.</strong></p>
<p>Answer:<br />
When developing features we either worked on a separate branch or a fork of the project, then when it was ready we made a PR. There we got a review from other group members and merged it to the main branch. In “special” moments this workflow was skipped.</p>
<h3>Question 10</h3>
<p><strong>Did you use DVC for managing data in your project? If yes, then how did it improve your project to have version</strong>
 <strong>control of your data. If no, explain a case where it would be beneficial to have version control of your data.</strong></p>
<p>Answer:<br />
We used data version control integrated into the google cloud storage bucket. Using two rules, the first one was that if an object with the same name was added to the bucket it would take over that spot and the other one would become noncurrent and thus would not be used. The second rule is that if the object has been noncurrent for more than 7 days it will be removed completely from the bucket. This helps us keep training on the most recent data. </p>
<h3>Question 11</h3>
<p><strong>Discuss you continues integration setup. What kind of CI are you running (unittesting, linting, etc.)? Do you test</strong>
 <strong>multiple operating systems, python version etc. Do you make use of caching? Feel free to insert a link to one of</strong>
 <strong>your github actions workflow.</strong></p>
<p>Answer:  <br />
We had 2 separate GitHub workflow files, one for code quality which executed ruff and mypy. Both improved our quality substantially. The other workflow file was for unit tests. Then we had a cloudbuild.yaml file to build, push and deploy our docker images.</p>
<h2>Running code and tracking experiments</h2>
<p>In the following section we are interested in learning more about the experimental setup for running your code and
 especially the reproducibility of your experiments.</p>
<h3>Question 12</h3>
<p><strong>How did you configure experiments? Did you make use of config files? Explain with coding examples of how you would</strong>
 <strong>run a experiment.</strong></p>
<p>Answer:<br />
We used Hydra with two options. One with a default values which could be called in the following way : <br />
<code > 
    <span style="background-color:#eee;">
        python train_model.py<br />
    </span><br />
</code ><br />
Which would run with default configuration once with our most promising hyperparameters.</p>
<p>The other one we utilized Hydras sweeper plugin Optuna where we created a grid search with the hyper parameters. That can be called with   </p>
<p><code >
    <span style="background-color:#eee;">
        python train_model.py —-multirun<br />
    </span>
</code ></p>
<p>Both options use the same file for convenience which can be found in src/conf/training_config.yaml</p>
<h3>Question 13</h3>
<p><strong>Reproducibility of experiments are important. Related to the last question, how did you secure that no information</strong>
 <strong>is lost when running experiments and that your experiments are reproducible?</strong></p>
<p>Answer:<br />
Our main reproducibility is within our data loader where it will always use the same images for training, validation and testing if the batch sizes and split percentage is the same since it uses the same random seed for shuffling. Secondary we utilized config files where if someone wanted to reproduce an experiment they would only need to send the hydra training_config.yaml file given that they are both using the same model.</p>
<h3>Question 14</h3>
<p><strong>Upload 1 to 3 screenshots that show the experiments that you have done in W&amp;B (or another experiment tracking</strong>
 <strong>service of your choice). This may include loss graphs, logged images, hyperparameter sweeps etc. You can take</strong>
 <strong>inspiration from . Explain what metrics you are tracking and why they are</strong>
 <strong>important.</strong></p>
<p>Answer:</p>
<!-- ![Weights and Biases image](figures/wandb.png) -->
<p><img src="figures/wandb.png" alt="drawing" width="1000"/></p>
<p>As can be seen we track the metrics loss (cross entropy loss in our case) and accuracy in training, validation and testing at every epoch. This informed us if we were overfitting, actually learning something and if they are better than previous ones. </p>
<h3>Question 15</h3>
<p><strong>Docker is an important tool for creating containerized applications. Explain how you used docker in your</strong>
 <strong>experiments? Include how you would run your docker images and include a link to one of your docker files.</strong></p>
<p>Answer:<br />
In our project we had 2 docker images. One for our model training (link: https://github.com/magnussig/ml-ops/blob/main/trainer.dockerfile) and another for our FastAPI that made predictions (https://github.com/magnussig/ml-ops/blob/main/api.dockerfile).</p>
<h3>Question 16</h3>
<p><strong>When running into bugs while trying to run your experiments, how did you perform debugging? Additionally, did you</strong>
 <strong>try to profile your code or do you think it is already perfect?</strong></p>
<p>Answer:<br />
We tried to get things to work locally, before adding to docker images or running things in the cloud. Some things were only testable in the cloud and those were the most difficult to iterate on, since some jobs/builds took up to 20 minutes to complete.</p>
<h2>Working in the cloud</h2>
<p>In the following section we would like to know more about your experience when developing in the cloud.</p>
<h3>Question 17</h3>
<p><strong>List all the GCP services that you made use of in your project and shortly explain what each service does?</strong></p>
<p>Answer: <br />
We used Google Storage to store and version our training data, as well as storing our trained models.<br />
We used Vertex AI to train our models.<br />
Cloud Build<br />
Container Registry<br />
Secret Manager </p>
<h3>Question 18</h3>
<p><strong>The backbone of GCP is the Compute engine. Explained how you made use of this service and what type of VMs</strong>
 <strong>you used?</strong></p>
<p>Answer:<br />
We did not use Compute Engine, we used Vertex AI for running the training docker image and saving the trained model in Cloud Storage.</p>
<h3>Question 19</h3>
<p><strong>Insert 1-2 images of your GCP bucket, such that we can see what data you have stored in it.</strong>
 <strong>You can take inspiration from.</strong></p>
<p>Answer:  </p>
<!-- ![Google Cloud Storage Buckets overview](figures/bucket.png) -->
<p><img src="figures/bucket.png" alt="drawing" width="600"/></p>
<!-- ![Folder structure of raw data ](figures/bucket_inside.png) -->
<p><img src="figures/bucket_inside.png" alt="drawing" width="600"/></p>
<h3>Question 20</h3>
<p><strong>Upload one image of your GCP container registry, such that we can see the different images that you have stored.</strong>
 <strong>You can take inspiration from .</strong></p>
<p>Answer:  </p>
<!-- ![GCP container registry](figures/registry.png) -->
<p><img src="figures/registry.png" alt="drawing" width="600"/></p>
<h3>Question 21</h3>
<p><strong>Upload one image of your GCP cloud build history, so we can see the history of the images that have been build in</strong>
 <strong>your project. You can take inspiration from.</strong></p>
<p>Answer:  </p>
<!-- ![GCP build history](figures/build.png) -->
<p><img src="figures/build.png" alt="drawing" width="600"/></p>
<h3>Question 22</h3>
<p><strong>Did you manage to deploy your model, either in locally or cloud? If not, describe why. If yes, describe how and</strong>
 <strong>preferably how you invoke your deployed service?</strong></p>
<p>Answer:<br />
 We used Vertex AI for running the training docker image and saving the trained model in Cloud Storage. We then deployed our FastAPI image that used the model in Cloud Storage to make predictions on images.</p>
<h3>Question 23</h3>
<p><strong>Did you manage to implement monitoring of your deployed model? If yes, explain how it works. If not, explain how</strong>
 <strong>monitoring would help the longevity of your application.</strong></p>
<p>Answer:<br />
We did not manage to implement monitoring. But if we would have then it would be two-fold. First would be implemented so that over time we could measure model performance mostly how much it degrades over time, which would be very helpful in determining the optimal time duration between re-trainings of the model. The second monitoring would be for the hardware which is mostly going to be used to minimize the cost while maintaining low latency and high uptime. This is usually measured with requests per timeunit and time duration of each request.</p>
<h3>Question 24</h3>
<p><strong>How many credits did you end up using during the project and what service was most expensive?</strong></p>
<p>Answer:<br />
In total we spent 2.12 USD but were not able to get access to cost breakdown per team member.  <br />
Service breakdown: <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cloud Storage : 0.92 USD <br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute Engine : 1.03 USD<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Vertex AI : 0.07 USD      </p>
<h2>Overall discussion of project</h2>
<p>In the following section we would like you to think about the general structure of your project.</p>
<h3>Question 25</h3>
<p><strong>Include a figure that describes the overall architecture of your system and what services that you make use of.</strong>
 <strong>Additionally in your own words, explain the</strong>
 <strong>overall steps in figure.</strong></p>
<p>Answer:  </p>
<!-- ![Overview of architecture](figures/overview.png) -->
<p><img src="figures/overview.png" alt="drawing" width="800"/></p>
<h3>Question 26</h3>
<p><strong>Discuss the overall struggles of the project. Where did you spend most time and what did you do to overcome these</strong>
 <strong>challenges?</strong></p>
<p>Answer:  <br />
One of the issues we had were related to authorization from within Google Cloud, for instance when trying to copy the trained model from within the container to a cloud bucket with “gsutil cp” we always got a 401 response from GCloud. In the end we used another method of copying with a python script.
Also we struggled in the beginning to visualize the big picture of the project, e.g. how to access the trained model to use it for prediction.</p>
<h3>Question 27</h3>
<p><strong>State the individual contributions of each team member. This is required information from DTU, because we need to</strong>
 <strong>make sure all members contributed actively to the project</strong></p>
<p>Answer:<br />
S223596 : Was in charge of initial model training loop and data<br />
S222720 : Was in charge of cloud deployment and co-charge of docker<br />
S202075 : Was in co-charge of docker<br />
S230883 : Was in charge of setting up initial cookie cutter, project and integration of fastAPI and ruff  </p>
<p>For the rest of the project the team equally contributed to f.x. training models, debugging and setting up logging with Weights and Biases.  </p>